{{/* Default Helm values for the app */}}
{{/* See schema for the appropriate app version here https://github.com/giantswarm/aws-nth-bundle/blob/main/helm/aws-nth-bundle/values.schema.json */}}
{{- define "defaultAwsNodeTerminationHandlerHelmValues" }}
awsNodeTerminationHandler:
  values:
    image:
      registry: {{ include "awsContainerImageRegistry" $ }}

    # Allow running on control plane nodes. On deletion, CAPI will first delete the worker nodes
    # and we still want aws-node-termination-handler, if it's even still running and the HelmRelease
    # not deleted yet, to take care of the last workers' EC2 lifecycle hooks since they otherwise
    # won't be completed, resulting in unnecessary waiting time before AWS can terminate the
    # instances (see `AWSMachinePool.spec.lifecycleHooks["aws-node-termination-handler"].heartbeatTimeout`).
    # This runs on workers by default but allows moving pods to control plane nodes. Requires
    # queue processing mode i.e. running as `Deployment`, not `DaemonSet`.
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: DoesNotExist
            weight: 10
    tolerations:
      - effect: NoSchedule
        operator: Exists
        key: node-role.kubernetes.io/control-plane

clusterID: {{ include "resource.default.name" $ }}
{{- if (.Values.global.connectivity.proxy).enabled }}
proxy:
  noProxy: "{{ include "cluster.connectivity.proxy.noProxy" (dict "global" $.Values.global "providerIntegration" $.Values.cluster.providerIntegration) }}"
  http: {{ .Values.global.connectivity.proxy.httpProxy | quote }}
  https: {{ .Values.global.connectivity.proxy.httpsProxy | quote }}
{{- end }}
global:
  image:
    registry: {{ include "awsContainerImageRegistry" $ }}
  podSecurityStandards:
    enforced: {{ .Values.global.podSecurityStandards.enforced }}
{{- end }}
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: {{ include "resource.default.name" $ }}-nth-bundle
  namespace: {{ $.Release.Namespace }}
  annotations:
    cluster.giantswarm.io/description: "{{ .Values.global.metadata.description }}"
  labels:
    cluster-apps-operator.giantswarm.io/watching: ""
    {{- include "labels.common" . | nindent 4 }}
spec:
  releaseName: aws-nth-bundle
  chart:
    spec:
      chart: aws-nth-bundle
      {{- $_ := set $ "appName" "aws-nth-bundle" }}
      version: {{ include "cluster.app.version" $ }}
      sourceRef:
        kind: HelmRepository
        name: {{ include "resource.default.name" $ }}-{{ include "cluster.app.catalog" $ }}
  kubeConfig:
    secretRef:
      name: {{ $.Values.global.managementCluster }}-kubeconfig
  interval: 5m
  install:
    remediation:
      retries: 30
  {{- $awsNodeTerminationHandlerHelmValues := (include "defaultAwsNodeTerminationHandlerHelmValues" .) | fromYaml -}}
  {{- $customAwsNodeTerminationHandlerHelmValues := $.Values.global.apps.awsNodeTerminationHandler.values -}}
  {{- if $customAwsNodeTerminationHandlerHelmValues }}
  {{- $awsNodeTerminationHandlerHelmValues = merge (deepCopy $customAwsNodeTerminationHandlerHelmValues) $awsNodeTerminationHandlerHelmValues -}}
  {{- end }}
  {{- if $awsNodeTerminationHandlerHelmValues }}
  values: {{- $awsNodeTerminationHandlerHelmValues | toYaml | nindent 4 }}
  {{- end }}
  {{- if $.Values.global.apps.awsNodeTerminationHandler.extraConfigs }}
  valuesFrom:
    {{- range $config := $.Values.global.apps.awsNodeTerminationHandler.extraConfigs }}
    - kind: {{ $config.kind }}
      name: {{ $config.name }}
      valuesKey: values
      optional: {{ $config.optional | default false  }}
    {{- end }}
  {{- end }}
